# Data sources

  The data from the weekly “Billboard Hot 100” charts and accompanying Spotify song characteristics for the music featured on the charts were taken and maintained by TidyTuesday [on GitHub](https://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-09-14). This data was provided in two separate files and required additional processing to combine (see Data Transformation for details). The Billboard chart data was scraped from the Billboard website and includes data over 3279 weeks from August 1958 to May 2021. For each of the 100 most popular songs each week, each song title is accompanied by its rank that week, its rank the previous week, the highest rank reached as of that week, total weeks on the chart as of that week, the recording artist name, and how many times the song has re-entered the Top 100 as of that week (“instance”). To uniquely identify each song, a “song_id” key was created by concatenating the song title and the performer. While the chart rank data is ordinal categorical, artist name and song title are simply nominal categorical. In total, this dataset was 327,895 rows by 10 columns.
  The song’s audio features data was provided by querying the Spotify API for each of the songs as they’re titled in the Billboard chart data. For each song in this dataset, the artist, numeric descriptive song features defined by Spotify, duration, album name, genre, Spotify’s popularity index, as well as Spotify-specific identifiers were provided. The song characteristics are numeric features generated by Spotify and their definitions from the API are listed below. The genre field for most songs listed multiple sub-genres and required additional cleaning to identify a “main” genre for each song for ease of analysis (see Data Transformation). Aside from the song features and song duration, which are numeric continuous variables, genre, artist, and album name are all nominal categorical data. In total this dataset was 29,503 rows by 22 columns. Since artist and song title were duplicated between both datasets, joining both into a single dataset and dropping unique Spotify identifiers (track url and preview url) resulted in a total of 30 columns and 327,895 rows.


```{r, echo=FALSE}
Feature <- c( 'acousticness', 'danceability', 'energy', 'instrumentalness', 'key', 'liveness', 'loudness', 'mode', 'speechiness', 'tempo', 'time_signature', 'valence')
Range <- c('0.0 - 1.0', '0.0 - 1.0', '0.0 - 1.0', '0.0 - 1.0', '-1 - 11', '-60 - 0', '0 or 1', '0.0 - 1.0', '0.0 - 1.0','-','3 -7', '0.0 - 1.0')
Description <- c("A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence",
                 "how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. 1.0 is most danceable.",
                 "A perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.",
                 "Predicts whether a track contains no vocals. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks.", 
                 "Integers map to pitches using standard Pitch CLass Notation. E.g 0 = C, 1 = C#, etc. If no key was detected, the value is -1",
                 "Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live",
                 "Overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks.", 
                 "Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.",
                 "Detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks",
                 "The overall estimated tempo of a track in beats per minute (BPM)",
                 "An estimated time signature. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of 3/4, to 7/4", 
                 "Describes the musical positiveness conveyed by a track. Tracks with high valence (closer to 1.0) sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).")

feats.data <- data.frame(Feature, Range, Description)
#options(knitr.table.format = "latex")
knitr::kable(feats.data, booktabs = TRUE, caption ="A table of the numeric Spotify song features ^[https://developer.spotify.com/documentation/web-api/reference/#/operations/get-several-audio-features]" )


```


  During the process of joining the two data sets and subsequent analysis, we noticed several problems. The primary issue was that there were some duplicate song instances in the Spotify dataset where the same song from a given artist was listed twice with some slight differences in the song characteristics. There were 117 such cases out of the 29,500 songs listed and we dropped one of the duplicates; this could have caused issues in our analysis if the instance dropped was a deprecated version of the song. During our analysis, we also noticed some incorrect genre labels in the Spotify dataset, notably where some songs were labeled with genres that did not yet exist when the song was released, likely due to Spotify’s retroactive labelling of its music. Also, each song was associated with multiple genres since Spotify only categorizes genre on the album-level. To simplify our analysis, a representative genre had to be chosen for each song (see Data Transformation); doing so may have led to some reduced granularity in the data. Finally, we noticed that in the cases where artists featured on another artists’ songs, the combination is registered as a unique artist. While we opted to leave the data this way instead of separating out each singer and attributing the song to multiple people for simplicity, we noticed that Spotify data was often missing for songs that had featured artists. This was likely because Spotify and Billboard handle these cases differently and looking up Spotify data using Billboard’s notation caused issues. As a result, the Spotify data (genre and musical characteristics) for many songs with featured artists is missing. 
